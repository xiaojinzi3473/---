{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## softmax和分类模型\n",
    "\n",
    "内容包含：\n",
    "    1. softmax回归的基本概念\n",
    "    2. 如何获取Fashion-MINST数据集和读取数据\n",
    "    3. softmax回归模型的从零开始实现，实现一个队Fashion-MNIST训练集中的图像数据进行分类的模型\n",
    "    4. 使用pytorch重新实现softmax回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax的基本概念\n",
    "\n",
    "- 分类问题\n",
    "通常使用离散的数值来表示类别\n",
    "\n",
    "- 权重矢量\n",
    "\n",
    "- 神经网络图\n",
    "softmax回归同线性回归一样，也是一个单层神经网络，输出层也是一个全连接层\n",
    "\n",
    "- 输出问题\n",
    "softmax运算符解决了以上两个问题。且不改变预测类别输出。\n",
    "\n",
    "- 计算效率\n",
    "    - 单样本矢量计算表达\n",
    "    - 小批量矢量计算表达式\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵损失函数\n",
    "\n",
    "\n",
    "对于样本$i$，我们构造向量$\\boldsymbol{y}^{(i)}\\in \\mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\\boldsymbol{\\hat y}^{(i)}$尽可能接近真实的标签概率分布$\\boldsymbol{y}^{(i)}$。\n",
    "\n",
    "- 平方损失估计  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}Loss = |\\boldsymbol{\\hat y}^{(i)}-\\boldsymbol{y}^{(i)}|^2/2\\end{aligned}\n",
    "$$\n",
    "  \n",
    "\n",
    "然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果$y^{(i)}=3$，那么我们只需要$\\hat{y}^{(i)}_3$比其他两个预测值$\\hat{y}^{(i)}_1$和$\\hat{y}^{(i)}_2$大就行了。即使$\\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如$\\hat y^{(i)}_1=\\hat y^{(i)}_2=0.2$比$\\hat y^{(i)}_1=0, \\hat y^{(i)}_2=0.4$的损失要小很多，虽然两者都有同样正确的分类预测结果。\n",
    "\n",
    "改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：\n",
    "\n",
    "\n",
    "$$\n",
    "H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},\n",
    "$$\n",
    "\n",
    "\n",
    "其中带下标的$y_j^{(i)}$是向量$\\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}{y^{(i)}}$为1，其余全为0，于是$H(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}) = -\\log \\hat y_{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。\n",
    "\n",
    "假设训练数据集的样本数为$n$，交叉熵损失函数定义为 \n",
    "$$\n",
    "\\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ),\n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\boldsymbol{\\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\\ell(\\boldsymbol{\\Theta}) = -(1/n) \\sum_{i=1}^n \\log \\hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\\ell(\\boldsymbol{\\Theta})$等价于最大化$\\exp(-n\\ell(\\boldsymbol{\\Theta}))=\\prod_{i=1}^n \\hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练和预测\n",
    "在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。在3.6节的实验中，我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取Fashion-MNIST训练集和读取数据\n",
    "在介绍softmax回归的实现前我们先引入一个多类图像分类数据集。它将在后面的章节中被多次使用，以方便我们观察比较算法之间在模型精度和计算效率上的区别。图像分类数据集中最常用的是手写数字识别数据集MNIST[1]。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST[2]。\n",
    "\n",
    "我这里我们会使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：\n",
    "1. torchvision.datasets: 一些加载数据的函数及常用的数据集接口；\n",
    "2. torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；\n",
    "3. torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；\n",
    "4. torchvision.utils: 其他的一些有用的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题\n",
    "\n",
    "选择题\n",
    "\n",
    "#### 1.softmax([100, 101, 102])的结果等于以下的哪一项\n",
    "\n",
    "A. softmax([10.0, 10.1, 10.2])\n",
    "\n",
    "B. softmax([-100, -101, -102])\n",
    "\n",
    "C. softmax([-2 -1, 0])\n",
    "\n",
    "D. softmax([1000, 1010, 1020])\n",
    "\n",
    "答案： C\n",
    "\n",
    "第一题解题思路：\n",
    "这是由于softmax函数的常数不变性，即$softmax(x)=softmax(x+c)$，推导如下：\n",
    "$$\n",
    "softmax(x_i)=\\frac{exp(x_i)}{\\sum_jexp(x_j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(softmax(x+c))_i=\\frac{exp(x_i+c)}{\\sum_j exp(x_j+c)}=\\frac{exp(c)exp(x_i)}{exp(c)\\sum_jexp(x_j)}=\\frac{exp(x_i)}{\\sum_jexp(x_j)}=(softmax(x))_i\n",
    "$$\n",
    "上面的exp(c)之所以可以消除，是因为$exp(a+b)=exp(a)*exp(b)$这个特性将exp(c)提取出来了。\n",
    "在计算softmax概率的时候，为了保证数值稳定性（numerical stability），我们可以选择给输入项减去一个常数，比如x的每个元素都要减去一个x中的最大元素。当输入项很大的时候，如果不减这样一个常数，取指数之后结果会变得非常大，发生溢出的现象，导致结果出现inf。\n",
    "*\n",
    "\n",
    "#### 2.对于本节课的模型，在刚开始训练时，训练数据集上的准确率低于测试数据集上的准确率，原因是\n",
    "\n",
    "A. 模型参数是在训练集上进行训练的，可能陷入了过拟合\n",
    "\n",
    "B. 训练集的样本容量更大，要提高准确率更难\n",
    "\n",
    "C. 训练集上的准确率是在一个epoch的过程中计算得到的，测试集上的准确率是在一个epoch结束后计算得到的，后者的模型参数更优\n",
    "\n",
    "答案： C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
