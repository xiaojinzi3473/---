{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾：\n",
    "\n",
    "- CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。\n",
    "- RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。\n",
    "\n",
    "为了整合CNN和RNN的优势，[\\[Vaswani et al., 2017\\]](https://d2l.ai/chapter_references/zreferences.html#vaswani-shazeer-parmar-ea-2017) 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。\n",
    "\n",
    "图10.3.1展示了Transformer模型的架构，与9.7节的seq2seq模型相似，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：\n",
    "1. Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。\n",
    "2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。\n",
    "3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。\n",
    "\n",
    "![Fig. 10.3.1 The Transformer architecture.](https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "$$\n",
    "Fig.10.3.1\\ Transformer 架构.\n",
    "$$\n",
    "\n",
    "\n",
    "在接下来的部分，我们将会带领大家实现Transformer里全新的子结构，并且构建一个神经机器翻译模型用以训练和测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "1.The Illustrated Transformer - http://jalammar.github.io/illustrated-transformer/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 习题\n",
    "\n",
    "选择题\n",
    "\n",
    "### 1.关于Transformer描述正确的是：\n",
    "\n",
    "A.在训练和预测过程中，解码器部分均只需进行一次前向传播。\n",
    "\n",
    "B.Transformer 内部的注意力模块均为自注意力模块。\n",
    "\n",
    "C.解码器部分在预测过程中需要使用 Attention Mask。\n",
    "\n",
    "D.自注意力模块理论上可以捕捉任意距离的依赖关系。\n",
    "\n",
    "**答案：D**\n",
    "答案解释\n",
    "\n",
    "选项1：训练过程1次，预测过程要进行句子长度次\n",
    "\n",
    "选项2：Decoder 部分的第二个注意力层不是自注意力，key-value来自编码器而query来自解码器\n",
    "\n",
    "选项3：不需要\n",
    "\n",
    "选项4：正确，因为自注意力会计算句子内任意两个位置的注意力权重\n",
    "\n",
    "### 2.在Transformer模型中，注意力头数为h，嵌入向量和隐藏状态维度均为d，那么一个多头注意力层所含的参数量是：\n",
    "\n",
    "A.$4hd^2$\n",
    "\n",
    "B.$(3h + 1)d^2$\n",
    "\n",
    "C.$4d^2$\n",
    "\n",
    "D.$3hd^2$\n",
    "\n",
    "**答案：A**\n",
    "\n",
    "答案解释\n",
    "\n",
    "参考MultiHeadAttention模块的定义。\n",
    "\n",
    "$h$个注意力头中，每个的参数量为$3d^2$，最后的输出层形状为$hd \\times d$，所以参数量共为$4hd^2$。\n",
    "\n",
    "### 3.下列对于层归一化叙述错误的是：\n",
    "\n",
    "A.层归一化有利于加快收敛，减少训练时间成本\n",
    "\n",
    "B.层归一化对一个中间层的所有神经元进行归一化\n",
    "\n",
    "C.层归一化对每个神经元的输入数据以mini-batch为单位进行汇总\n",
    "\n",
    "D.层归一化的效果不会受到batch大小的影响\n",
    "\n",
    "**答案：C**\n",
    "\n",
    "答案解释\n",
    "\n",
    "批归一化（Batch Normalization）才是对每个神经元的输入数据以mini-batch为单位进行汇总\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
